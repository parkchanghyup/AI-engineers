# 1.딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
---
딥러닝이란 **여러가지 층을 가진 인공신경망을 사용하여 머신러닝 학습을 수행 하는것** 이다.  
딥러닝은 엄밀히 말해 머신러닝에 포함되는 개념이다. 


# 2.왜 갑자기 딥러닝이 부흥했을까요?  
---


# 3.마지막으로 읽은 논문은 무엇인가요? 설명해주세요  
---
- `efficient-net`  
올해 3월 부스트캠프를 진행하며 같이 공부한 조원들과 pytorch 연습을 위해 데이콘에서 주관한 컴퓨터 비전 경진대회에 참가 함. 
이미지분류 분야에서 높은 성능을 내는 efficient-net을 알게 되었고 그때 읽어 보았음.  

 모델의 정확도를 높일 때, 일반적으로 (1) 모델의 깊이, (2) 너비, (3) 입력 이미지의 크기를 조절합니다. 기존에는 이 세 가지를 수동으로 조절하였기 때문에, 최적의 성능과 효율을 얻지 못했습니다. EfficientNet은 3가지를 효율적으로 조절할 수 있는 compound scaling 방법을 제안합니다. 깊이, 너비, 입력 이미지 크기가 일정한 관계가 있다는 것을 실험적으로 찾아내고, 이 관계를 수식으로 만듭니다  



# 4.Cost Function과 Activation Function은 무엇인가요?  

- `cost function` : 모델이 현재 예측을 얼마나 잘하고 있는지 알아야 학습 방향을 어느 방향으로, 어떤식으로 개선할지 판단할 수 있다. 이때, 예측값과 실제값의 차이에 대한 함수를 cost function이라고 한다.  
- `Activation function`:  데이터를 예측하기 위해 선형 모델을 사용할 수 있지만 선형 모델을 사용하면 층을 깊게 쌓아도 의미가 없고, 복잡한 데이터에 대해서 적절한 예측을 못함. 이를 처리하기 위해서 **비선형 모델**이 필요함.  
선형 모델을 비선형 모델로 만들어주는 역할을 하는 함수가 바로 활성화 함수이다.      
선형 모델은 깊게 쌓아도 의미가 없는게 적절한 선형 모델로 표현 할 수 있기 때문이다. 그러나 비선형모델은 깊게 쌓을 수록 더욱 복잡하게 작동하고 다른 비선형 모델로 표현 할 수 없다.  


# 5.Tensorflow, PyTorch 특징과 차이가 뭘까요?
Tensorflow와 Pytorch의 가장 큰 차이점은 딥러닝을 구현하는 패러다임이 다르다는 것.  
Tensorflow는 `Define-and-Run` 인 반면에, Pytorch는 `Define-by-Run` 이다.  

- Define and Run (Tensorflow) : 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식으로 이는 계산 그래프를 명확히 보여주면서 실행시점에 데이터만 바꿔줘도 되는 유연함을 장점으로 갖지만, 그 자체로 비직관적이다.
- Define by Run (PyTorch) : 선언과 동시에 데이터를 집어넣고 세션도 필요없이 돌리면 되기 때문에 코드가 간결하고 난이도가 낮은 편.

# 6. Data Normalization은 무엇이고 왜 필요한가요?  

Data Normalization 이란 변수들의 Scale(분포)를 조절하여 균일하게 만드는 방법이다.  


||1km 이내 초등학교| 인근지역 1개월 당 경범죄 발생 건수의 평균| 인근 지하철 역까지의 거리(m)|집 가격(만원)|
|---|---|---|---|---| 
|최소값|0|8|30|10,000|
|평균값|1.5|23.1|80|100,000|
|최댓값|5|90|300|500,000|

위 데이터에서 집 가격을 보면 적게는 1억원에서 비싸게는 50억원의 집이 있다. 우리가 만들고자 하는 머신러닝 모델이 선형 회귀 모델이라고 생각해보자  $WX+b$ 형태의 식이 될 텐데, W와 b같은 파라미터가 다른 열(1km 이내 초등학교의 개수 등)로 부터 10000~5000000의 큰 값을 만들어내려면 매우 큰 값이 필요할 것이고, 이는 학습률(Learning Rage)를 불필요하게 크게 만드는 요인이 될 것이다.  
<br/> 

이런 문제는 예측해야할 데이터 뿐만 아니라 입력 데이터에서도 발생한다. 1km 이내 초등학교 데이터는 작으면 0, 커봤자 5이다. 그러나 인근 지하철 역까지의 거리 데이터는 30 ~ 300까지 너무나도 큰 데이터를 갖는다. 이런 경우, 학습을 진행했을때, 실제로는 인근 지하철 역의 거리보다도 학군이 중요함에도, 큰 값을 갖는 지하철 역 데이터에 가중치가 편향되는 문제가 생길 수 있다.   
**머신러닝 모델이 선입견을 갖게 되는 것이다.**


#7 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)  
# 8.오버피팅일 경우 어떻게 대처해야 할까요?  
#9 하이퍼 파라미터는 무엇인가요?  
#10 Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?  
#11 볼츠만 머신은 무엇인가요?  
#12 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?  
#13 Non-Linearity라는 말의 의미와 그 필요성은?  
#14 ReLU로 어떻게 곡선 함수를 근사하나?  
#15 ReLU의 문제점은?  
#16 Bias는 왜 있는걸까?  
#17 Gradient Descent에 대해서 쉽게 설명한다면?  
#18 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?  
#19 GD 중에 때때로 Loss가 증가하는 이유는?  
#20 중학생이 이해할 수 있게 더 쉽게 설명 한다면?  
#21 Back Propagation에 대해서 쉽게 설명 한다면?  
#22 Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?  
#23 GD가 Local Minima 문제를 피하는 방법은?  
#24 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?  
#25 Training 세트와 Test 세트를 분리하는 이유는?  
#26 Validation 세트가 따로 있는 이유는?  
#27 Test 세트가 오염되었다는 말의 뜻은?  
#28 Regularization이란 무엇인가?  
#29 Batch Normalization의 효과는?  
#30 Dropout의 효과는?  
#31 BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?  
#32 GAN에서 Generator 쪽에도 BN을 적용해도 될까?  
#33 SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?  
#34 SGD에서 Stochastic의 의미는?  
#35 미니배치를 작게 할때의 장단점은?  
#36 모멘텀의 수식을 적어 본다면?  
#37 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?  
#38 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?  
#39 Back Propagation은 몇줄인가?  
#40 CNN으로 바꾼다면 얼마나 추가될까?  
#41 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?  
#42 CNN이 아닌 MLP로 해도 잘 될까?  
#43 마지막 레이어 부분에 대해서 설명 한다면?  
#44 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?  
#45 만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까?  
#46 딥러닝할 때 GPU를 쓰면 좋은 이유는?  
#47 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?  
#48 GPU를 두개 다 쓰고 싶다. 방법은?  
#49 학습시 필요한 GPU 메모리는 어떻게 계산하는가?  
#50 TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?  
#51 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?  
  
