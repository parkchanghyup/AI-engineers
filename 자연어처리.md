# 자연어 처리(NLP)

## One Hot 인코딩에 대해 설명해주세요 
One Hot 인코딩이란 카테고리형 변수를 숫자형 변수로 변환 시켜주는 방법이다.   

One Hot 인코딩을 적용 하는 방법에는 판다스 라이브러리의 get_dummies() 메소드나, sklearn의 OneHotEncoder 메소드를 이용해서 적용 할 수 있다.  
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdpCAiq%2FbtqzXiyopA0%2FcYknntmiZk0ZF5IM1bnafk%2Fimg.png)


## POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요?

POS(Part-Of-Speech) 태깅은 문장 내의 단어들을 형태소 분석하여 태그 붙여주는 방법을 말한다. 간단하게 형태소를 분석하는 방법으로는 한국어의 경우 konlpy 라이브러리를 이용하면되고 영어의 경우 nltk를 이용하면 된다.

## 문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?

“Apple” 단어를 포함한 문장들과 과일, 회사 Label을 pair한 학습 데이터를 만든다. 단어를 판단할 때 문맥을 구성하는 다른 토큰들을 고려할 수 있는 모델을 선정한다. 예를 들어 n-gram으로 주변에 출현한 토큰을 저장하고, 과일일때와 회사일때의 주변 토큰을 통계적으로 방법을 이용할 수 있고, LSTM이나 Transformer, BERT와 같은 비교적 최신 모델을 이용하여 class가 2개인 텍스트 분류 모델을 훈련할 수 있다.

## 영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?

영어 텍스트와 번역 목표 언어가 pair로 구성된 데이터를 구비한다.
2021년 현재 비교적 높은 성능을 보이는 BERT, ELECTRA 등의 Transformer기반 모델의 pre-trained Language Model을 구비한다.
1에서 준비한 데이터 쌍을 이용하여 fine-tune 학습을 수행한다.
REST API를 지원하는 web-framework를 이용하여 서비스를 구성한다.
Docker를 이용하여 container 빌드하여 서비스한다.

## Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요?

stop word란 문장내에서 불필요하다고 판단되는 단어들의 집합입니다.  
분석 모델링에 오히려 방해가 될 수 있기 때문에 제거해줍니다.


## TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?
문서 내에서 특정 단어의 중요성을 계산하기 위해 사용되는 점수 
- TF : 문서 내에서 특정 단어의 등장 빈도
- IDF : 즉정 단어가 얼마나 문서 집합 전체에서 희소하게 등장하는지 측정
TF-IDF는 TF * IDF로 계산된다. 


## RNN에 대해 설명해주세요
기존의 인공신경망 구조는 입력과 출력이 독립적으로 처리되는데, 이는 순서와 상관 관계가 있는 데이터(예: 문장, 음악, 시계열 데이터 등)를 처리하기에는 적합하지 않다. 
RNN은 이러한 문제를 해결하기 위해 고안된 신경망 구조로, **내부에 순환 구조를 가지고 있어** 이전 단계의 출력을 현재 단계의 입력으로 전달한다.
## LSTM은 왜 유용한가요?
RNN 구조 특성상 긴 문장에 대해서 제대로 처리하지 못한다. 
이를 개선하기 위해 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)와 같은 변형된 RNN 구조가 개발되었다. 이들은 기존의 RNN에 메모리 셀을 도입하여 긴 문장을 처리할때 먼저 등장한 단어에 대한 정보를 기억하고(장기 의존성을 처리), 불필요한 정보를 삭제하거나 중요한 정보를 보존하는 방식으로 학습을 진행하게된다.

## n-gram은 무엇일까요?
텍스트나 시퀀스 데이터에서 연속적으로 나타나는 n개의 연속적인 요소들로 구성된 조합을 의미한다.     
가장 일반적으로 사용되는 n-gram은 bigram(2-gram)과 trigram(3-gram)이다.  
예를 들어, "Hello, how are you?"라는 문장을 bigram으로 분해하면 ["Hello, how"], ["how, are"], ["are, you"]와 같은 연속된 단어 쌍으로 나뉘게 된다. trigram으로 분해하면 ["Hello, how, are"], ["how, are, you"]와 같이 연속된 세 개의 단어로 나뉘게 된다.

## Word2Vec의 원리는?
    그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는?
    그 그림에서 오른쪽 파라메터들의 의미는 무엇일까?
    남자와 여자가 가까울까? 남자와 자동차가 가까울까?
    번역을 Unsupervised로 할 수 있을까?
