# 1.딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
---
딥러닝이란 **여러가지 층을 가진 인공신경망을 사용하여 머신러닝 학습을 수행 하는것** 이다.  
딥러닝은 엄밀히 말해 머신러닝에 포함되는 개념이다. 


# 2.왜 갑자기 딥러닝이 부흥했을까요?  
---


# 3.마지막으로 읽은 논문은 무엇인가요? 설명해주세요  
---
- `efficient-net`  
올해 3월 부스트캠프를 진행하며 같이 공부한 조원들과 pytorch 연습을 위해 데이콘에서 주관한 컴퓨터 비전 경진대회에 참가 함. 
이미지분류 분야에서 높은 성능을 내는 efficient-net을 알게 되었고 그때 읽어 보았음.  

 모델의 정확도를 높일 때, 일반적으로 (1) 모델의 깊이, (2) 너비, (3) 입력 이미지의 크기를 조절합니다. 기존에는 이 세 가지를 수동으로 조절하였기 때문에, 최적의 성능과 효율을 얻지 못했습니다. EfficientNet은 3가지를 효율적으로 조절할 수 있는 compound scaling 방법을 제안합니다. 깊이, 너비, 입력 이미지 크기가 일정한 관계가 있다는 것을 실험적으로 찾아내고, 이 관계를 수식으로 만듭니다  



# 4.Cost Function과 Activation Function은 무엇인가요?  
---
- `cost function` : 모델이 현재 예측을 얼마나 잘하고 있는지 알아야 학습 방향을 어느 방향으로, 어떤식으로 개선할지 판단할 수 있다. 이때, 예측값과 실제값의 차이에 대한 함수를 cost function이라고 한다.  
- `Activation function`:  데이터를 예측하기 위해 선형 모델을 사용할 수 있지만 선형 모델을 사용하면 층을 깊게 쌓아도 의미가 없고, 복잡한 데이터에 대해서 적절한 예측을 못함. 이를 처리하기 위해서 **비선형 모델**이 필요함.  
선형 모델을 비선형 모델로 만들어주는 역할을 하는 함수가 바로 활성화 함수이다.      
선형 모델은 깊게 쌓아도 의미가 없는게 적절한 선형 모델로 표현 할 수 있기 때문이다. 그러나 비선형모델은 깊게 쌓을 수록 더욱 복잡하게 작동하고 다른 비선형 모델로 표현 할 수 없다.  


# 5.Tensorflow, PyTorch 특징과 차이가 뭘까요?
---
Tensorflow와 Pytorch의 가장 큰 차이점은 딥러닝을 구현하는 패러다임이 다르다는 것.  
Tensorflow는 `Define-and-Run` 인 반면에, Pytorch는 `Define-by-Run` 이다.  

- Define and Run (Tensorflow) : 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식으로 이는 계산 그래프를 명확히 보여주면서 실행시점에 데이터만 바꿔줘도 되는 유연함을 장점으로 갖지만, 그 자체로 비직관적이다.
- Define by Run (PyTorch) : 선언과 동시에 데이터를 집어넣고 세션도 필요없이 돌리면 되기 때문에 코드가 간결하고 난이도가 낮은 편.

# 6. Data Normalization은 무엇이고 왜 필요한가요?  
---
Data Normalization 이란 변수들의 Scale(분포)를 조절하여 균일하게 만드는 방법이다.  


||1km 이내 초등학교| 인근지역 1개월 당 경범죄 발생 건수의 평균| 인근 지하철 역까지의 거리(m)|집 가격(만원)|
|---|---|---|---|---| 
|최소값|0|8|30|10,000|
|평균값|1.5|23.1|80|100,000|
|최댓값|5|90|300|500,000|

위 데이터에서 집 가격을 보면 적게는 1억원에서 비싸게는 50억원의 집이 있다. 우리가 만들고자 하는 머신러닝 모델이 선형 회귀 모델이라고 생각해보자  $WX+b$ 형태의 식이 될 텐데, W와 b같은 파라미터가 다른 열(1km 이내 초등학교의 개수 등)로 부터 10000~5000000의 큰 값을 만들어내려면 매우 큰 값이 필요할 것이고, 이는 학습률(Learning Rage)를 불필요하게 크게 만드는 요인이 될 것이다.  
<br/> 

이런 문제는 예측해야할 데이터 뿐만 아니라 입력 데이터에서도 발생한다. 1km 이내 초등학교 데이터는 작으면 0, 커봤자 5이다. 그러나 인근 지하철 역까지의 거리 데이터는 30 ~ 300까지 너무나도 큰 데이터를 갖는다. 이런 경우, 학습을 진행했을때, 실제로는 인근 지하철 역의 거리보다도 학군이 중요함에도, 큰 값을 갖는 지하철 역 데이터에 가중치가 편향되는 문제가 생길 수 있다.   
**머신러닝 모델이 선입견을 갖게 되는 것이다.**


# 7. 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)  
# 8.오버피팅일 경우 어떻게 대처해야 할까요?  
---
### Regularzation 
모델의 일반화 성능을 높이기 위하여 모델에 제약을 주며 학습하여 overfitting을 방지하는 방법.  
1. Early stopping
    - training loss는 계속 낮아지더라도 validation loss는 올라가는 시점을 overfitting으로 간주하여 학습을 종료하는 방법
2. Parameter norm penalty (weight decay)
    - 비용함수에 제곱을 더하거나(L2 규제) 절댓값을 더해서(L1 규제) weight의 크기에 페널티를 부과하는 방법
3. Data augmentation
    - 훈 련 데이터의 개수가 적을 때, 데이터에 인위적으로 변화를 주어 훈련 데이터의 수를 늘리는 방법
4. Noise robustness
    - 노이즈나 이상치같은 엉뚱한 데이터가 들어와도 흔들리지 않는(roubst 한) 모델을 만들기 위해 input data나 weight에 일부러 노이즈를 주는 방법
5. Dropout
    - 딥러닝 모델의 각 계층 마다 일정 비율의 뉴런을 임의로 정해 drop 시키고 나머지 뉴런만 학습하도록 하는 방법
    - 매 학습마다 drop 되는 뉴런이 달라지기 때문에 서로 다른 모델들을 앙상블 하는 것과 같은 효과가 있다.
    - dropout는 **학습 시에만 적용하고, 추론 시에는 적용하지 않는다.**
6. Batch normalization
    - 활성화함수의 활성화값 또는 출력값을 정규화 하는 방법
    - 각 hidden layer에서 정규화를 하면서 입력분포가 일정하게 되고, 이에 따라 Learning rate를 크게 설정해도 괜찮아진다. 결과적으로 학습 속도가 빨라지는 효과가 있다.

# 9.하이퍼 파라미터는 무엇인가요? 
--- 
하이퍼 파라미터는 모델링 할 때, 사용자가 **직접 셋팅해주는 값**을 뜻한다.  
정해진 최적의 값은 없으며, 직접 실험을 해보거나가 경험을 바탕으로 설정한다. 하이퍼 파라미터의 종류에는 학습률, 배치 사이즈 등이 있다.  
하이퍼 파라미터 튜닝 기법에는 Grid Search, Random Search, Bayesian Optimization 등이 있다. 


# 10. Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?  
---

딥러닝에서 가중치를 잘 초기화하지 않는다면 기울기 소실이나  local minima 등의 문제를 야기할 수 있기 때문에 매우 중요하다. 
- Lecun Initialzation : 들어오는 노드 수에 대해 정규 분포와 균등 분포를 따르는 방법이 있다.
- Xavier Initialization : Lecun과 비슷함.  들어오는 노드 수와 나가는 노드 수에 의존하고, 적절한 상수값도 발견하여 사용하는 방법
- He Initialization : ReLU와 함께 많이 사용되는 방법으로, LeCun 방법과 같지만 상수를 다르게 하였다.  


# 11. 볼츠만 머신은 무엇인가요?  
---- 

# 12 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?  
---
sigmodi보다 ReLU를 많이 쓰는 가장 큰 이유는 **기울기 소실 문제** 떄문이다. 기울기는 연쇄볍칙(Chain Rule)에 의해 국소적 미분값을 누적 곱을 시키는데, sigmoid의 경우 기울기가 항상 0과 1사이의 값이므로 이 값을 연쇄적으로 곱하게 되면 0 에 수렴할 수 밖에 없다. 반면 ReLU는 값이 양수일 때, 기울기가 1이므로 연쇄 곱이 1보다 작아지는 것을 어느 정도 막 아 줄 수 있다.   

다만, ReLU는 값이 음수이면, 기울기가 0이기 때문에 일부 뉴런이 죽을 수 있다는 단점이 존재한다. 이를 보완한 활성화 함수를 Leaky ReLU가 있다.  

# 13. Non-Linearity라는 말의 의미와 그 필요성은?  
---
딥러닝에서 비선형 관계는 활성화 함수를 이용하면서 표현할 수 있다. 그럼 비선형 관계
즉, 활성화 함수가 왜 필요할까? 바로 **활성화 함수를 사용해 여러 층을 쌓아서 더 복잡한 표현을 하기 위해서**이다. 화서오하 함수가 $h(x) = cx$인 선형 함수라고 생각해보자. 이 때 n 개의 층을 쌓았다고 할 때, 모델은 $y = h^n(x) = c^n(x)$로 나타낼 수 있다. $c^n = k$라는 상수로 치환하면 결국 1층을 가진 신경망과 동일하다. 그렇기 때문에 비선형인 활성화 함수가 필요한 것이다.  

# 14. ReLU로 어떻게 곡선 함수를 근사하나?  
---
ReLU를 여러 개 결합하면, 특정 지점에서 특정 각도만큼 선형 함수를 구부릴 수 있다. 이 성질을 이용하여 곡선 함수 뿐만 아니라 모든 함수에 근사를 할 수 있게 된다.

# 15. ReLU의 문제점은?  
---
ReLU의 가장 큰 문제점은 바로 **죽은 뉴런(Dead Neurons)**이다. ReLU는 결과값이 음수인 경우 모두 0으로 취급하는데, back propagation시 기울기에 0이 곱해져 해당 부분의 뉴런은 죽고 그 이후의 뉴런 모두 죽게 된다. 이를 해결한 Leaky ReLU는 값이 음수일 때 조금의 음의 기울기를 갖도록 하여 뉴런이 조금이라도 기울기를 갖도록 한다.

# 16. Bias는 왜 있는걸까?  
--- 
편향(Bias)는 활성화 함수가 왼쪽 혹은 오른쪽으로 이동할 수 있게 해준다. 가중치(weight)는 활서오하 함수의 가파른 정도. 즉, 기울기를 조절하는 반면, 편향(bias)는 **활성화 함수를 움직임으로써 데이터에 더 잘 맞도록 한다**

# 17. Gradient Descent에 대해서 쉽게 설명한다면?  

#18 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?  
#19 GD 중에 때때로 Loss가 증가하는 이유는?  
#20 중학생이 이해할 수 있게 더 쉽게 설명 한다면?  
#21 Back Propagation에 대해서 쉽게 설명 한다면?  
#22 Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?  
#23 GD가 Local Minima 문제를 피하는 방법은?  
#24 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?  
#25 Training 세트와 Test 세트를 분리하는 이유는?  
#26 Validation 세트가 따로 있는 이유는?  
#27 Test 세트가 오염되었다는 말의 뜻은?  
#28 Regularization이란 무엇인가?  
#29 Batch Normalization의 효과는?  
#30 Dropout의 효과는?  
#31 BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?  
#32 GAN에서 Generator 쪽에도 BN을 적용해도 될까?  
#33 SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?  
#34 SGD에서 Stochastic의 의미는?  
#35 미니배치를 작게 할때의 장단점은?  
#36 모멘텀의 수식을 적어 본다면?  
#37 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?  
#38 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?  
#39 Back Propagation은 몇줄인가?  
#40 CNN으로 바꾼다면 얼마나 추가될까?  
#41 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?  
#42 CNN이 아닌 MLP로 해도 잘 될까?  
#43 마지막 레이어 부분에 대해서 설명 한다면?  
#44 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?  
#45 만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까?  
#46 딥러닝할 때 GPU를 쓰면 좋은 이유는?  
#47 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?  
#48 GPU를 두개 다 쓰고 싶다. 방법은?  
#49 학습시 필요한 GPU 메모리는 어떻게 계산하는가?  
#50 TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?  
#51 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?  
  
