# 1.딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

딥러닝이란 **여러가지 층을 가진 인공신경망을 사용하여 머신러닝 학습을 수행 하는것** 이다.  
딥러닝은 엄밀히 말해 머신러닝에 포함되는 개념이다. 



# 2.왜 갑자기 딥러닝이 부흥했을까요?  

딥러닝의 경우 높은 성능을 내기 위해서는 대용량의 데이터가 필요함.  
빅데이터 시대가 되면서 대용량의 데이터를 확보 할 수 있게 된 것이 딥러닝의 부흥과 직접적으로 연관 되어 있다고 생각됨.  

# 3.마지막으로 읽은 논문은 무엇인가요? 설명해주세요  

- `efficient-net`  
올해 3월 부스트캠프를 진행하며 같이 공부한 조원들과 pytorch 연습을 위해 데이콘에서 주관한 컴퓨터 비전 경진대회에 참가 함. 
이미지분류 분야에서 높은 성능을 내는 efficient-net을 알게 되었고 그때 읽어 보았음.  

 모델의 정확도를 높일 때, 일반적으로 (1) 모델의 깊이, (2) 너비, (3) 입력 이미지의 크기를 조절합니다. 기존에는 이 세 가지를 수동으로 조절하였기 때문에, 최적의 성능과 효율을 얻지 못했습니다. EfficientNet은 3가지를 효율적으로 조절할 수 있는 compound scaling 방법을 제안합니다. 깊이, 너비, 입력 이미지 크기가 일정한 관계가 있다는 것을 실험적으로 찾아내고, 이 관계를 수식으로 만듭니다  



# 4.Cost Function과 Activation Function은 무엇인가요?  

- `cost function` : 모델이 현재 예측을 얼마나 잘하고 있는지 알아야 학습 방향을 어느 방향으로, 어떤식으로 개선할지 판단할 수 있다. 이때, 예측값과 실제값의 차이에 대한 함수를 cost function이라고 한다.  
- `Activation function`:  데이터를 예측하기 위해 선형 모델을 사용할 수 있지만 선형 모델을 사용하면 층을 깊게 쌓아도 의미가 없고, 복잡한 데이터에 대해서 적절한 예측을 못함. 이를 처리하기 위해서 **비선형 모델**이 필요함.  

선형 모델을 비선형 모델로 만들어주는 역할을 하는 함수가 바로 활성화 함수이다.      
선형 모델은 깊게 쌓아도 의미가 없는게 적절한 선형 모델로 표현 할 수 있기 때문이다. 그러나 비선형모델은 깊게 쌓을 수록 더욱 복잡하게 작동하고 다른 비선형 모델로 표현 할 수 없다.  


# 5.Tensorflow, PyTorch 특징과 차이가 뭘까요?

Tensorflow와 Pytorch의 가장 큰 차이점은 딥러닝을 구현하는 패러다임이 다르다는 것.  
Tensorflow는 `Define-and-Run` 인 반면에, Pytorch는 `Define-by-Run` 이다.  

- Define and Run (Tensorflow) : 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식으로 이는 계산 그래프를 명확히 보여주면서 실행시점에 데이터만 바꿔줘도 되는 유연함을 장점으로 갖지만, 그 자체로 비직관적이다.
- Define by Run (PyTorch) : 선언과 동시에 데이터를 집어넣고 세션도 필요없이 돌리면 되기 때문에 코드가 간결하고 난이도가 낮은 편.

# 6. Data Normalization은 무엇이고 왜 필요한가요?  

Data Normalization 이란 변수들의 Scale(분포)를 조절하여 균일하게 만드는 방법이다.  


||1km 이내 초등학교| 인근지역 1개월 당 경범죄 발생 건수의 평균| 인근 지하철 역까지의 거리(m)|집 가격(만원)|
|---|---|---|---|---| 
|최소값|0|8|30|10,000|
|평균값|1.5|23.1|80|100,000|
|최댓값|5|90|300|500,000|

위 데이터에서 집 가격을 보면 적게는 1억원에서 비싸게는 50억원의 집이 있다. 우리가 만들고자 하는 머신러닝 모델이 선형 회귀 모델이라고 생각해보자  $WX+b$ 형태의 식이 될 텐데, W와 b같은 파라미터가 다른 열(1km 이내 초등학교의 개수 등)로 부터 10000~5000000의 큰 값을 만들어내려면 매우 큰 값이 필요할 것이고, 이는 학습률(Learning Rage)를 불필요하게 크게 만드는 요인이 될 것이다.  
<br/> 

이런 문제는 예측해야할 데이터 뿐만 아니라 입력 데이터에서도 발생한다. 1km 이내 초등학교 데이터는 작으면 0, 커봤자 5이다. 그러나 인근 지하철 역까지의 거리 데이터는 30 ~ 300까지 너무나도 큰 데이터를 갖는다. 이런 경우, 학습을 진행했을때, 실제로는 인근 지하철 역의 거리보다도 학군이 중요함에도, 큰 값을 갖는 지하철 역 데이터에 가중치가 편향되는 문제가 생길 수 있다.   
**머신러닝 모델이 선입견을 갖게 되는 것이다.**


# 7. 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)  
![]('image/activation.jpg)

# 8.오버피팅일 경우 어떻게 대처해야 할까요?  

### Regularzation 
모델의 일반화 성능을 높이기 위하여 모델에 제약을 주며 학습하여 overfitting을 방지하는 방법.  
1. Early stopping
    - training loss는 계속 낮아지더라도 validation loss는 올라가는 시점을 overfitting으로 간주하여 학습을 종료하는 방법
2. Parameter norm penalty (weight decay)
    - 비용함수에 제곱을 더하거나(L2 규제) 절댓값을 더해서(L1 규제) weight의 크기에 페널티를 부과하는 방법
3. Data augmentation
    - 훈 련 데이터의 개수가 적을 때, 데이터에 인위적으로 변화를 주어 훈련 데이터의 수를 늘리는 방법
4. Noise robustness
    - 노이즈나 이상치같은 엉뚱한 데이터가 들어와도 흔들리지 않는(roubst 한) 모델을 만들기 위해 input data나 weight에 일부러 노이즈를 주는 방법
5. Dropout
    - 딥러닝 모델의 각 계층 마다 일정 비율의 뉴런을 임의로 정해 drop 시키고 나머지 뉴런만 학습하도록 하는 방법
    - 매 학습마다 drop 되는 뉴런이 달라지기 때문에 서로 다른 모델들을 앙상블 하는 것과 같은 효과가 있다.
    - dropout는 **학습 시에만 적용하고, 추론 시에는 적용하지 않는다.**
6. Batch normalization
    - 활성화함수의 활성화값 또는 출력값을 정규화 하는 방법
    - 각 hidden layer에서 정규화를 하면서 입력분포가 일정하게 되고, 이에 따라 Learning rate를 크게 설정해도 괜찮아진다. 결과적으로 학습 속도가 빨라지는 효과가 있다.


# 9.하이퍼 파라미터는 무엇인가요? 

하이퍼 파라미터는 모델링 할 때, 사용자가 **직접 셋팅해주는 값**을 뜻한다.  
정해진 최적의 값은 없으며, 직접 실험을 해보거나가 경험을 바탕으로 설정한다. 하이퍼 파라미터의 종류에는 학습률, 배치 사이즈 등이 있다.  
하이퍼 파라미터 튜닝 기법에는 `Grid Search`, `Random Search`, `Bayesian Optimization` 등이 있다. 


# 10. Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?  

딥러닝에서 가중치를 잘 초기화하지 않는다면 기울기 소실이나  local minima 등의 문제를 야기할 수 있기 때문에 매우 중요하다. 
- Lecun Initialzation : 들어오는 노드 수에 대해 정규 분포와 균등 분포를 따르는 방법이 있다.
- Xavier Initialization : Lecun과 비슷함.  들어오는 노드 수와 나가는 노드 수에 의존하고, 적절한 상수값도 발견하여 사용하는 방법
- He Initialization : ReLU와 함께 많이 사용되는 방법으로, LeCun 방법과 같지만 상수를 다르게 하였다.  


# 11. 볼츠만 머신은 무엇인가요?  
---- 

# 12 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?  

sigmodi보다 ReLU를 많이 쓰는 가장 큰 이유는 **기울기 소실 문제** 떄문이다. 기울기는 연쇄볍칙(Chain Rule)에 의해 국소적 미분값을 누적 곱을 시키는데, sigmoid의 경우 기울기가 항상 0과 1사이의 값이므로 이 값을 연쇄적으로 곱하게 되면 0 에 수렴할 수 밖에 없다. 반면 ReLU는 값이 양수일 때, 기울기가 1이므로 연쇄 곱이 1보다 작아지는 것을 어느 정도 막 아 줄 수 있다.   

다만, ReLU는 값이 음수이면, 기울기가 0이기 때문에 일부 뉴런이 죽을 수 있다는 단점이 존재한다. 이를 보완한 활성화 함수를` Leaky ReLU`가 있다.  

# 13. Non-Linearity라는 말의 의미와 그 필요성은?  

딥러닝에서 비선형 관계는 활성화 함수를 이용하면서 표현할 수 있다.   
그럼 비선형 관계 즉, 활성화 함수가 왜 필요할까? 바로 **활성화 함수를 사용해 여러 층을 쌓아서 더 복잡한 표현을 하기 위해서**이다.  
활성화 함수가 $h(x) = cx$인 선형 함수라고 생각해보자. 이 때 n 개의 층을 쌓았다고 할 때, 모델은 $y = h^n(x) = c^n(x)$로 나타낼 수 있다. $c^n = k$라는 상수로 치환하면 결국 1층을 가진 신경망과 동일하다. 그렇기 때문에 비선형인 활성화 함수가 필요한 것이다.  

# 14. ReLU로 어떻게 곡선 함수를 근사하나?  

ReLU를 여러 개 결합하면, 특정 지점에서 특정 각도만큼 선형 함수를 구부릴 수 있다. 이 성질을 이용하여 곡선 함수 뿐만 아니라 모든 함수에 근사를 할 수 있게 된다.

# 15. ReLU의 문제점은?  

ReLU의 가장 큰 문제점은 바로 **죽은 뉴런(Dead Neurons)**이다. ReLU는 결과값이 음수인 경우 모두 0으로 취급하는데, back propagation시 기울기에 0이 곱해져 해당 부분의 뉴런은 죽고 그 이후의 뉴런 모두 죽게 된다. 이를 해결한 Leaky ReLU는 값이 음수일 때 조금의 음의 기울기를 갖도록 하여 뉴런이 조금이라도 기울기를 갖도록 한다.

# 16. Bias는 왜 있는걸까?  

편향(Bias)는 활성화 함수가 왼쪽 혹은 오른쪽으로 이동할 수 있게 해준다. 가중치(weight)는 활성화 함수의 가파른 정도. 즉, 기울기를 조절하는 반면, 편향(bias)는 **활성화 함수를 움직임으로써 데이터에 더 잘 맞도록 한다**

# 17. Gradient Descent에 대해서 쉽게 설명한다면?  

Gradient Descent는 어떤 함수의 극소점을 찾기 위해 gradient 반대 방향으로 이동해 가는 방법이다.  
딥러닝에서 Loss fucntion을 최소화 시키기 위해 파라미터에 대해 Loss function을 미분하여 그 기울기값(gradient)를 구하고, 경사가 하강하는 방향(loss function이 최소가 되는 지점)으로 파라미터 값을 점진적으로 찾기위해 사용 된다.  

**Gradient Descent의 문제점**  

- 적절한 step size(learning rate)
    - step size가 큰 경우 한 번 이동하는 거리가 커지므로 빠르게 수렴할 수 있다는 장점이 있다. 하지만 step size를 너무 크게 설정해 버리면 최소값을 계산하도록 수렴하지 못하고 함수 값이 계속 커지는 방향으로 최적화가 진행될 수 있다. 
    - 한편 Step size가 너무 작은 경우 밠나하지는 않을 수 있지만 최적의 x를 구하는데 소요되는 시간이 오래 걸린다는 단점이 있다.
- local minima 문제
    - gradient descent 알고리즘을 시작하는 위치는 매번 랜덤하기 때문에 어떤 경우에는 local minima에 빠져 계속 헤어나오지 못하는 경우도 생긴다. 



# 18.  왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?  


# 19. GD 중에 때때로 Loss가 증가하는 이유는?   

local minima에 들어갔다가 나오는 경우일 것이다.
실제로 사용되는 Gd에서는 local minima 문제를 피하기 위해 Momentum 등의 개념을 도입한 RMSprop, Adam 등의 optimization 전략을 사용한다.  (부스트캠프 DAY12)

각 optimizaiont 전략에 따라 gradient가 양수인 방향으로도 parameter update step를 가져가는 경우가 생길 수 있으며, 이 경우에는 Loss가 일시적으로 증가할 수 있다.  


# 21. Back Propagation에 대해서 쉽게 설명 한다면?  

역전파 알고리즘은 Loss에 대한 입력값의 기울기(미분값)을 출력층 layer에서부터 계산하여 거꾸로 전파시키는 것이다.  

이렇게 거꾸로 전파시켜서 최종적으로 출력층에서의 output값에 대한 입력층에서의 input data의 기울기 값을 구할 수 있다.  

이 과정에서 **chain rule**가 사용된다.  

출력층 바로전 layter 에서부터 기울기(미분값)을 계산하고 이를 점점 거꾸로 전파시키면서 전 layer들에서의 기울기와 서로 곱하는 형식으로 나아가면 최종적으로 출력층의 output에 대한 입력층에서의 input의 기울기(미분값)을 구할 수 있다.  

<br/>  

역전파 알고리즘이 해결한 문제가 바로 파라미터가 매우 많고 layer가 여러개 있을때 가중치 w와 b를 학습시키기 어려웠다는 문제이다.  

이는 역전파 알고리즘으로 각 layer에서 기울기 값을 구하고 그 기울기 값을 이용하여 Gradient descent 방법으로 가중치w와 b를 update시키면서 해결되었다.  


# 22. Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?  

Optimization 전략을 통해 local Minima 문제를 어느정도 피할 수 있기 때문이다.

# 23. GD가 Local Minima 문제를 피하는 방법은?  

Local minima 문제를 피하는 방법으로는 Momentum, Nesterov Accelerated Gradient(NAG), Adagrad, Adadelta, RMSprop, Adam 등이 있다.
# 24. 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?  


Global Minima가 정확히 어디에 존재하는지는 알 수 없다. 다만, 학습에 사용하지 않은 Test Dataset에 대한 성능을 평가하는 것으로 모델이 Global Minima에 가까운지 유추할 수 있다.

# 25. Training 세트와 Test 세트를 분리하는 이유는?  

모델은 데이터에 대해 예측값을 만들고 정답과 비교하며 업데이트 되면서 학습이 된다.
글너데 학습 데이터에 대해서는 좋은 성능을 낸다 하더라도 본 적 없는 데이터에 대해서는 잘 대응하지 못하는 **오버피팅** 문제가 생긴다면 좋은 모델이 아니다.  
이를 막기 위해 학습된 모델이 처음 보는 데이터에도 좋은 성능을 내는지 판단하기 위한 수단으로 test 셋을 따로 만든다.  

# 26. Validation 세트가 따로 있는 이유는?  

모델을 학습시키고 test 데이터를 통해 모델의 일반화 성능을 파악하고, 다시 모델에 새로운 시도를 하고 test 데이터를 통해 성능을 파악한다고 생각해보자.  

이 경우, 모델은 결국 test 데이터에도 오버피팅 되어서 다시 처음 보는 데이터를 주면 좋은 성능을 보장할 수 없게 된다.  

이 문제를 막기 위해 validation 셋을 사용한다. validtaion 셋을 통해 모델의 성능을 평가하고 하이퍼파라미터 등을 수정하는 것이다.  

- validation data -> 모의고사  
- test data -> 수능  

# 27. Test 세트가 오염되었다는 말의 뜻은?    

test 데이터가 모델 학습에 사용 되 었다는 의미

# 28. Regularization이란 무엇인가? 

모델의 오버피팅을 막고 처음 보는 데이터에도 잘 예측하도록 모델을 학습시키는 방법을 regularization 이라함.

# 29. Batch Normalization의 효과는?  

배치 정규화(Batch Normalization)은 학습 시 미니배치 단위로 입력의 분포가 평균이 0, 분산이 1이 되도록 정규화한다. 더불어 $gamma$로 스케일과 $beta$로 이동 변환을 수행한다. 이렇게 배치 정규화를 사용하면 다음과 같은 효과를 얻을 수 있다.
- 장점 1 : 기울기 소실/폭발 문제가 해결되어 큰 학습률을 설정할 수 있어 학습속도가 빨라진다.
- 장점 2 : 항상 입력을 정규화시키기 때문에 가중치 초깃값에 크게 의존하지 않아도 된다.
- 장점 3 : 자체적인 규제(Regularization) 효과가 있어 Dropout이나 Weight Decay와 같은 규제 방법을 사용하지 않아도 된다.


# 30. Dropout의 효과는?  

dropout는 **설정된 확률 $p$만큼 은닉층(hidden layer)에 있는 뉴런을 무작위로 제거하는 방법**으로, 오버피팅을 방지하기 위한 방법 중 하나이다. (정확히는 출력을 0으로 만들어 더이상의 전파가 되지 않도록 한다.) 드롭아웃은 학습 때마다 무작위로 뉴런을 제거하므로 매번 다른 모델을 학습시키는 것으로 해석할 수 있다. 그리고 추론 시 출력에 제거 확률 p를 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는 효과를 얻을 수 있다.
# 31 BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?  
# 32 GAN에서 Generator 쪽에도 BN을 적용해도 될까?  
# 33 SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?  
# 34 SGD에서 Stochastic의 의미는?  
# 35 미니배치를 작게 할때의 장단점은?  

- 장점
    - 한 iteration의 계산량이 적어지기 때문에 step당 속도가 빨라진다.
    - 적은 graphic ram으로 학습이 가능하다.
- 단점
    - 데이터 전체의 경향을 반영하기 어렵다. 업데이트를 항상 좋은 방향으로 하지만은 않는다. 

# 36 모멘텀의 수식을 적어 본다면? 

# 37 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?  

2-layer 신경망을 구현한다고 했을 때, 100줄 이내로 만들 수 있다.

# 38 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?  
# 39 Back Propagation은 몇줄인가?  
# 40 CNN으로 바꾼다면 얼마나 추가될까?  
# 41 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?  

30분 이내

# 42 CNN이 아닌 MLP로 해도 잘 될까?  

Convolution 레이어는 receptive field를 통해 이미지의 위치 정보까지 고려할 수 있다는 장점이 있다.  
반면 MLP는 모두 Fully connected 구조이므로 이미지의 특징을 이해하는데 픽셀마다 위치를 고려할 수 없게 된다. 따라서 MNIST 분류기에서 MLP를 사용하면 CNN을 사용했을 때보다 성능이 낮다.  


# 43 마지막 레이어 부분에 대해서 설명 한다면?  

MNIST 분류기는 Convolution 레이어를 깊게 쌓으며 숫자 이미지의 작은 특징부터 큰 특징까지 파악한다. 
마지막 레이어, Fully connecte 레이어는 이미지 데이터의 특징을 취합하여 10개의 숫자 중 적절한 숫자로 분류하는 역할을 한다.  
만약 더 많은 레이블에 대해 분류해야 한다면 마지막 레이어의 out dimension을 그에 맞게 설정하면 된다.
# 44 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?  

학습에 사용하는 loss function을 BEC loss로 하고 validation 데이터를 이용한 모델 검증에서 MSE loss를 사용 하면 된다.  


# 46 딥러닝할 때 GPU를 쓰면 좋은 이유는?  

GPU는 부동 소수점 연산을 수행하는 많은 코어가 있어 수 많은 연산을 **병렬처리**할 수 있다. 
또한 CPU 보다 더 큰 메모리 대역폭을 가지고 있기 때문에 **큰 데이터를 더 효율적으로 빠르게 처리** 할 수 있다. 

# 47 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?  

batch size가 작거나, 데이터의 크기가 애초에 작아서 gpu를 100% 사용할 필요가 없기 때문이 아닐까 ?

# 48 GPU를 두개 다 쓰고 싶다. 방법은?  
Pytorch의 경우 `torch.nn.DataParallel`을 사용하여 여러 개의 GPU를 사용할 수 있다.

- torch.device를 cuda로 설정한다.
- nn.DataParallel을 사용하여 모델을 감싼다.
- 모델을 model.to(device)를 사용하여 GPU로 보낸다.

# 49 학습시 필요한 GPU 메모리는 어떻게 계산하는가?  

Pytorch를 기준으로 볼 때 something.to('cuda')로 변환하는 모든 것들을 생각해보면 된다. 보통 GPU로 올리는 것은 모델과 데이터셋이므로,** (모델의 크기 + 데이터의 크기 × 배치 크기)**로 학습시 필요한 메모리 크기를 계산할 수 있다.

# 51. 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?  

사람은 처음 보는 물건(새 레이블)에 대해 조금만 봐도 다른 것과 이 물건을 구분해 낼수 있다.  
하지만 뉴럴넷은 이 물건을 구분해내기 위해서는 이 물건에 대한 많은 데이터를 학습해야 한다.

`one-shot learing`은 뉴럴넷도 새로운 레이블을 지닌 데이터가 적을 때(one-shot 에서는 한 개)에도 모델이 좋은 성능을 내도록 사용되는 방법이다.  

이를 위해서는 기존에 다른 레이블의 많은 데이터를 학습하여 데이터의 특성을 잘 이해하는 `pretrained` 모델이 필요하다.  

학습된 모델에 새로운 레이블의 데이터 하나를 던져 주면 모델은 데이터의 특성에 대한 이해를 바탕으로 이레 이블에 대해서도 이해를 하게 된다.  
  
